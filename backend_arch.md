# 后端架构设计

标签（空格分隔）： 服务器设计[cloutropy]
---

撰写人：梅刚

---
                                                
## 设计约束
1. 多机房并存。多机房存在并非基于系统可靠性要求，而是服务客户独立子网存在的缘故
2. 跨子网的终端节点(SDK)之间 **不优先** 互联 甚至禁止互联，但子网机房中的服务器可以与中心机房互联
3. 在策略和物理限制允许的情况下，可跨机房借调种子资源
4. 在子网机房与中心机房失去连接的极端情形下，子网仍能保持常规的服务，包括正常的视频播放和p2p传输，可以忍受短暂的中心机房策略控制的缺失

## 当前面临的问题
1. 计算中心定位不明，承担过多的繁杂的任务，且以实时计算模式为主，可能有些计算并无实时性需求，计算的性价比不高，甚至可能的趋势是超越我们的计算能力。所以，本着好钢用在刀刃上原则，可以进行分类拆解，将某些计算任务归还给职能服务器，某些计算采用离线系统分析甚至更廉价的MR计算系统。
2. 功能服务器对中心的依赖过大，其行为速度在很大程度上依赖于自身与中心服务器的交互和反馈效率，而中心作为庞大的数据计算系统，恐难以支撑所有服务器的响应要求，故而极有可能导致中心对功能的服务器的指导指令过期，不合时宜，即指令到底的时候已经形式有变。总之，中心更适合做高吞吐高延时的工作。
3. 功能服务器内聚度过低，服务器被拆分得过细，导致一些信息在服务期间同步和共享成本极高甚至不予考虑。也导致对系统的理解和运维部署上的难度加大。
4. 数据和计算过度分离，显著增加系统模块间的数据传输，比如筛选种子的行为，需将存储于外部系统的信息同步到本地，在从中选择下发。此外，存储系统本身(redis/mongodb及其数量)显得有些庞杂和重度。同时，外部存储服务器所能支撑的的存储形式即数据结构并不一定符合计算需求。
5. 机器配置不妥，都采用低配而堆数量的方式，以迎合所谓的node的io密集而计算稀疏的特性[单线程异步IO]，导致系统效率低下而机器数量增长显著。
6. 出现了数据副本过多的倾向，一些个数据辗转反侧的流转，空费消耗，可考虑合并部分存储和查询系统。 
7. 历史和传统。基于我们一向的打补丁的开发模式，一般都是在已有系统上微调和添加新的服务器角色，承接历史，继续用队列和存储系统做胶水粘接新模块………………还有一点，各子系统的设计者主要专注于本系统，并且没有分享本部分设计和心得的习惯，导致系统间的融合度有限，甚至是凑活着，而各部分的参与者也对其他部件的认知度较低。其次，我们做事略显急躁，没有架构清楚便开始动手实现，结果要么参与的人要么配合的人在实施过程中出现迷惑。
    
## 设计目标
> 总体的设计思想就是: 区域自治 + 宏观调控。职能部件负责状态维护和交互实施，智能部件负责监控和适量干预。具体说来，在本系统中如此体现：种子资源维护/节点状态维护/任务下发、监督和调整/推送服务器控制等功能皆由各功能服务器配合完成，无需中心参与。中心则负责根据全网的运行状态做跨网资源调度，比如种子资源补给以及网内任务下发。当然，作为补充，**中心机房甚至可以提供智能计算的访问接口**，比如分中心想知道在自己当前的负载情况下，预计未来一个小时的负载增加量和概率。 

1. 子网采集的所有关键终端数据以及子网服务器的运维数据都通过专门管道同步到中心机房，包括SDK服务质量、HTTP响应质量、流量、种子资源分布等等

2. 子网负责业务需求的支撑和策略的实施，中心机房负责策略形成、全网监控、指标统计、商业智能、跨网资源调度协调

3. 中心机房向子机房提供3中类型的数据同步方式
    - 主动同步。以订阅或者轮询的形式，实时的将预订信息同步到目标机房。适合于任务下发这样的主动控制场景。
    - 被动请求。子机房需要指定信息的时候，向中心机房发出同步数据的请求，属于按需分配模式。比如，在种子资源出现不充足的时候(当然不用等到已经出现不足的时候，在匮乏度达到一定阈值的时候亦可)，分机房可以向中心机房发出同步某些文件的种子资源的需求。

        
## 整体架构

> 本部分主要描绘系统在跨机房的上的交互行为,具体的细节上的交互细节不在此详述,包括不直接参与交互的内部构件。同时必须指出，这里设计表达上是凸显机房间的交互，并没有单独描述那些直接部署在中心机房的功能服务器的场景，区别仅仅在于省去了机房间同步的部分，也就是**退化版本**，故而不再赘述。

1. 主要构件图
    ![云熵后台的跨机房设计.png-47.1kB](http://static.zybuluo.com/error12345678/tf8x6k5sgvrs1wkqu5sp8gva/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E7%9A%84%E8%B7%A8%E6%9C%BA%E6%88%BF%E8%AE%BE%E8%AE%A1.png)

    角色说明:
    ### 中心机房(常规的数据流流转在此不再赘述)
    - 计算系统
      泛指各种具体数据处理和只能策略实现的分布式计算工具,包括 Spark实时计算(Spark Streaming)，Spark离线计算，SQL查询。
    - 存储系统
        存储设施上主要为HDFS(以Hive支撑模式存储,现为AVRO文件格式), HBase. 该部分主要维护这样几类信息：常规原始数据，统计结果，监控数据，全网资源镜像(节点在线状况、种子资源等)，指定各区域下发的任务等等。根据各类数据的格式特点选择对应的存储形式。
        之前的系统中因Hive无法提供低响应的实时交互查询，故而将数据导出到MySQL中提供查询。而本设计中使用的HBase可提供实时查询，故而可不用再将数据导出到MySQL系统。当然为了服务的一致性，可包装Restful接口提供相同服务
    - 队列
        此处为Kafka, 提供数据汇入的高速缓存之用,日志上报和控制信息传递接口走此通道。
    - 跨机房数据传输工具
      Kafka当前考虑 Apache Flume和confluent的Mirror maker。HBase则考虑使用CopyTable (其通过HLog实现增量同步)
    
    ### 分机房
    - TrackerServer(ts)
        p2p服务的核心服务器之一，其利用存储系统(outside or inside)维护节点和文件种子信息,并根据一定的策略向请求的SDK分配seed资源。SDK所需的p2p服务主要由该服务器提供接口支持,所以SDK的相关交互主要与该服务器发生(部分交互由stun转发完成)
    - 推送服务器（PushMgr）
        编码的CDN
    - 任务管理系统(TaskMgr)
        其同步并维护从控制中心(中心机房的策略部件)获取的任务，本根据自身策略以及与其相关的推送服务器的状态，构建任务的执行计划，并在执行过程中动态调整任务的优先级和顺序，并配合推送服务器的负载状态调整任务下发模式
    - 队列
        此处为Kafka, 作为与中心机房交互的媒介，以及方便与本机房某些模块的通信
    - 跨机房同步工具
        当前考虑Apache Flume和confluent的Mirror maker
    
2. 机房之间的主要交互
    - 分机房实时上报到中心。ts接受的资源汇报以及节点上下线信息除了更新本地的状态维护之外，将副本实时投递到队列中，同步工具则会自动将这些信息从队列同步到中心机房的队列中,中心相应的计算框架便能感知和处理这些信息了
    ![云熵后台架构---分机房向中心机房的同步[种子汇聚].png-30.3kB](http://static.zybuluo.com/error12345678/2201fwbnoa8lx3un0nvw4agj/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E6%9E%B6%E6%9E%84---%E5%88%86%E6%9C%BA%E6%88%BF%E5%90%91%E4%B8%AD%E5%BF%83%E6%9C%BA%E6%88%BF%E7%9A%84%E5%90%8C%E6%AD%A5%5B%E7%A7%8D%E5%AD%90%E6%B1%87%E8%81%9A%5D.png)
    - 分机房按需向中心机房请求资源。如子网发现自己的种子数量不够充裕，同时其节点又可以与其他子网通信，这样分机房可向中心调拨所需文件的种子列表.
    - 分机房使用中心机房的智能计算接口。因该请求的及时性要求可能较高，可考虑建立单独的请求链路，而不与数据同步的大管道并用。当然，根据业务需要了，及时性要求不高，仍可复用大管道。中心机房通过该接口主要提供以下功能：需要密集计算或者特殊算法才能得到的结果、获取其他子网的运行状态和资源状态、应急通知(据此影响中心对本地区的控制策略)。简而言之，就是提供计算能力服务和控制服务管道。
    - 中心机房向分机房下发任务。主要包括推送任务和对功能服务器的调控任务。
    - 中心日常性的向子机房同步一些日常性的计算结果。比如本区域前一天的流量,p2p比例,热门文件top100等等，以供本区域的策略优化
    ![云熵后台控制---数据流通道.png-16.2kB](http://static.zybuluo.com/error12345678/n21k9d8u312f0ywzfb0r7rr8/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E6%8E%A7%E5%88%B6---%E6%95%B0%E6%8D%AE%E6%B5%81%E9%80%9A%E9%81%93.png)

## 典型场景描述及局部设计
1. 分机房的日志上报
![云熵后台架构---日志采集.png-17.7kB](http://static.zybuluo.com/error12345678/fo567fq4titezpdgq7e6xqt1/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E6%9E%B6%E6%9E%84---%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86.png)
此处废弃了之前flume扮演的collector中转角色,以提高端到端的高可靠性。因为collector虽然是多机部署，但是在宕机的时候，残留在collector通道(flume channel)中的  数据只有在机器重新恢复的时候才能被输送出来。如果再也启动不了那这部分数据也就消失了。既然利用Kafka直连可以解决问题就不去改造collector重复造轮子了。
至于Kafka之间的Topic同步工具，先选定使用Mirror maker。

2. 中心机房向分机房传递控制信息
![云熵后台架构---控制信息传导.png-19.9kB](http://static.zybuluo.com/error12345678/eyfsghprqfcrpvbf1m9l6waa/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E6%9E%B6%E6%9E%84---%E6%8E%A7%E5%88%B6%E4%BF%A1%E6%81%AF%E4%BC%A0%E5%AF%BC.png)

3. 中心机房向分机房传递任务等批量的大量结构化或非结构化数据
![云熵后台设计---任务下发流程.png-44.3kB](http://static.zybuluo.com/error12345678/vfwjlkpvsenl350xbu4304ff/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E8%AE%BE%E8%AE%A1---%E4%BB%BB%E5%8A%A1%E4%B8%8B%E5%8F%91%E6%B5%81%E7%A8%8B.png)
基本流程即是，决策系统(包括计算系统和任务控制系统)计算出了针对特定机房的任务数据之后，存入到HBase中相应机房的任务表。专门的的跨机房HBase同步工具会自动将特定的数据同步到远端的HBase存储中，同时利用控制渠道向分机房发送任务下达信号，对端的任务控制器(TaskMgr)收到控制信号(亦可通过某种轮询机制实现)后便可制定任务的执行计划并实施。任务计划的实施仰赖于一些p2pserver的配合，但由TaskMgr主导，后文会进一步描述。

4. TrackerServer架构
![云熵后台架构---TrackerServer架构.png-31.5kB](http://static.zybuluo.com/error12345678/0yeyhc75j11srhasfkn88g29/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E6%9E%B6%E6%9E%84---TrackerServer%E6%9E%B6%E6%9E%84.png)
服务器的主体角色由TrackerServer-Master和TrackerServer-Worker组成。Master为避免单点问题，可添加副本并实时同步数据，并使用Zookeeper等工具实现在Master节点失效时的Master选举替换。下面先介绍下各个角色的职责:
    - Tracker-Master. 主要维护节点和文件在worker节点中的分布信息，即某个文件的信息或者某个节点的状态信息(主要记录在线状态)被记录在哪个node上。FileLocator负责维护文件所在集群节点,PeerLocator负责维护终端节点信息所在集群节点。Balancer主要用来平衡集群各节点的访问负载，主要通过控制文件副本数量来调节的。比如发现某些热门文件的访问频率较高，而且副本数量又只有一个，于是请求种子的请求被密集的调度到了这唯一副本所在的节点。这个时候Balancer可调控该文件的副本数量，让某个其他节点也同步这个文件的相关信息(尤其是种子信息),这样来自终端的请求会被部分调度到新分配的节点上，故而降低了之前节点的负载。当然，当访问频度降低到一定阈值的时候，Balancer亦可降低副本数量以均衡信息的分布，避免信息冗余度带来的存储消耗和查询效率的降低。此外，Balancer亦可设计更复杂的调度算法，如同时根据节点的访问频度和其维护的信息量(影响单次查询效率)来综合作为节点负载的指标，籍次作为平衡的出发点，努力做到在整个集群中的负载均衡。
    - TrackerServer-Worker. 主要处理种子下发请求。FileIndex,用以记录文件的相关信息，尤其是当前该文件被持有的节点列表(即种子列表)。Peer_Alive_Info，维护节点状态，尤其是在线状态，并通过超时器(堆或树)控制节点下线。Peer_Hosting_Files,记录该节点拥有的文件列表，当该节点下线时则可快速更改文件索引中的节点列表，相当于和FileIndex建立的双向索引。但是这种双向关心并不一定在同一节点上，也就是说某peer拥有的文件并不一定存在于本机的FileIndex中。
    - GateServer. 网关服务器，唯一对外交互的角色，其可以部署多台，并且都是无状态的。可以达到两个效果，一是可以合并SDK与TS的交互，只要一次交互即可，GateServer根据请求与TS的多个角色进行进行两次甚至多次交互(如通过重定向302实现),并将结果返回给SDK。二则可以分担请求和应答流量，避免结果都由Master代理返回。
    
    ------------------
    由于上述设计过于复杂，此处提供简化版本：
    ![云熵后台架构---TrackerServer架构简化版.png-32.5kB](http://static.zybuluo.com/error12345678/vi95vmsfhiiyw2uwqzzw9dmv/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E6%9E%B6%E6%9E%84---TrackerServer%E6%9E%B6%E6%9E%84%E7%AE%80%E5%8C%96%E7%89%88.png)
    与之前设计的主要区别在于简化了Master的设计，Master不再考虑具体文件的分布和机器的负载，而只是根据简单的运算规则将指定的资源映射到某一个或几个Worker节点上。比如 Hash(fileid) -> 2 -> node-2-1,node-2-2,node-2-3，GateServer根据返回的节点列表随机选择访问节点，如果访问的节点宕机了，则从剩余列表中选择。

5. 与终端的长连接的维持
![云熵后台架构---终端长连接的维持.png-26.6kB](http://static.zybuluo.com/error12345678/yfrqc2xxg7rspsdaf31268xx/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E6%9E%B6%E6%9E%84---%E7%BB%88%E7%AB%AF%E9%95%BF%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%BB%B4%E6%8C%81.png)
SDK间歇性的向Stun发送心跳包，以维持其路由器上的端口对Stun服务器的开放状态。Stun将收到的心跳包转发给TS系统，TS据此记录节点的在线状态，并做计时处理，超时而未有心跳的节点会被TS系统判定为离线状态。同时，TS会记录该节点连接使用的IP和端口号，以及该节点所建立连接的Stun服务器。这样在某个服务器需要向指定的peer发送数据的时候，便可以通过TS查询到该Peer连接的具体Stun服务器，然后**让该Stun服务器代理向指定的IP和端口发送数据**。

6. 任务的执行计划的大致实施步骤
![云熵后台架构---推送任务的执行计划 .png-31.3kB](http://static.zybuluo.com/error12345678/a37yxgtwe6dvjsm3hvkl4649/%E4%BA%91%E7%86%B5%E5%90%8E%E5%8F%B0%E6%9E%B6%E6%9E%84---%E6%8E%A8%E9%80%81%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%20.png)
先说明一下，虚线绘制的PushMgr其实和末端那个PushMgr是一个东西，只是为了绘制方便而已，因为隔得太远了。正如上一个topic所说，TaskMgr通过TS查询到指定的peer连接信息之后，转告响应的Stun服务器发送任务指令。


## 关键问题解决
>0.最大的挑战是什么？是现有的服务器架构的简易性与分布式系统的高可靠性设计的矛盾。分布式系统废了九牛二虎之力达到极高的可靠性，可是只要整个数据链路中有一个环节出现了风险，整个体系的可靠性就大打折扣。这就好比我们建立了保护严密的几座原油仓库，可是如果输油管道都是一些脆弱的网络，势必带来原油泄漏的风险。举个例子，kafka和hdfs都是简装的系统，但如果我们自行编写的从kafka导出数据到hdfs的工具不具有健壮性和可扩展性便可能带来数据丢失、单点失效，延时过大性能低下等问题。而要实现这样的效果，往往不可避免的引入常用的复杂的分布式技术。所幸，现在的生态相对丰富。言外之意就是说，我们在设计的时候应尽可能的复用现有的成熟的分布式解决方案，不可轻易的用自建建议系统替代。

1. 跨机房的数据同步。对于类似日志这样的线性结构的彼此独立的流式消息传递，直接通过flume连接彼此对端的kafka就可以了。但是对于大体积的树状结构的传输怎么办呢？比如一个文件关联的种子和机房信息需要传递到分机房，这样的操作最好能在语义上支持原子操作，使得对端提取信息时获得的是完整的数据单元。怎么搞呢？与采取的基础设施有关，比设计中用HBase，所以借用HBase的远程工具实现。
    
2. 跨机房的网络连接问题。因内部服务器均采用私网或者局域网IP，无法与同样处于私有网络的其他服务器直接建立连接，故而起码需要一些手段让部分参与交互的部件能够互通。此处提供三种建议方案：
    - 将不同的私网络加入到相同的VPN网络中，VPN服务器最好由中心机房提供以保证连接效果。该方案能够给服务器部署带来最大的便利，不用明显感知跨网络的存在。
    - 给全部参与交互的边界机器分配公网IP。该方案需要耗费公网IP。部署上则只需注意目标服务器的IP的设置。
    - 由网关服务器或者路由器或者是负载均衡部件做端口转发(NAT)，这样可省去许多公网IP资源的消耗。部署上则需要注意目标服务器的IP地址和端口号的设置，之前给不同目标服务器设置的相同端口需要修改为针对相同IP的不同端口。

3. 端到端的可靠性。全部链路采用高可靠性分布式系统，同时严格限制链路长度，避免冗余数据传递环节。正如前文所以所提及的废弃flume做为collector的设计，只将flume用于数据采集源头服务器。这里也面临了一个选择，在flume进行日志采集的时候是用File-Channel还是Memory-Channel？File-Channel可持久化数据，宕机重启之后可原样恢复，但是效率相对较低，而Memory-Channel特性恰好相反。考虑到采集的日志来源于已经落盘的日志文件，因宕机或者程序崩溃丢失的残留在channel中数据是可以恢复的，故而设计上可在flume的sink处设计类似check-point的机制，记录日志文件的读取进度，重启后可恢复。同时，在采集源头按一定的规则为每一条日志生成唯一标识，以实现全程的幂等性。

4. 性能与效率问题
    - 收缩服务器角色种类，显现低耦合同时高内聚，降低通信代价。
    - 逐步构建分布式系统以实现可伸缩性，并广泛采用成熟的开源构件。
    - 对于单机而言，尽可能提高机器配置而降低机器数量，并采用高并发度设计以提高硬资源的利用率。使用生态成熟的语言，有效而深度的利用开源界的贡献，降低开发成本。
    - 不同种类或者不同区域的业务(可能因客户群的要求所致, 如安徽电信)，尽可能的做到逻辑隔离，而不是物理隔离，这样才能与效率提高资源的利用率，同时降低运维成本，甚至避免运维工作无限扩大的灾难性后果

5. SDK升级问题
    老板，我们还做嵌入SDK的虚拟机吗？
        
## 知乎(Q&A)
1. 为何对于任务这样的消费信息不通过队列直接消费了事，而要存储到HBase中？

    直接的队列消费模式更适合于不care消息的结构关系和优先级，直接拿到变消费掉的情形，比如HTTP请求到达后可缓存在管道中，WEB服务器直接从管道中提取请求直接应答即可。但是推送任务却不同，计算系统计算的那个时候无法东西后续的场景而制定好严格的照做便好的事件流。直接从管道消费并实施会依赖任务在管道中的顺序。比如举个简单的例子，从管道中拿到某个下发到指定节点的任务后发现该节点当前不在线根本没法实施任务下发。还有可能出现更复杂的场景需求，比如任务管理器根据当时的push服务器的负载和文件请求频度，认为此刻优先下发某些指定的任务可能效率更为优越，于是便可从HBase中进行查询提取。此外，HBase可通过MapReduce间接提供一些计算能力，对数据进行一些简单的二次处理。

    此外，引入HBase还有更广泛的考虑。此前的离线计算任务中，是将计算结果存入到ODPS(等效于HDFS或者Hive)，然后再将所需的计算结果导出到MySQL中供外部查询提取之用。实施上Hive本身就可以提供数据查询的功能，为何需要导出到MySQL中呢？源于Hive的查询实质上是转换成MapReduce进行即时计算，故而不能提供低延时交互或高频查询之用，此处引入的HBase却可以这样的功用。
    
    我们也不要忘记，之前就因为我们限定了数据的结构化特征（输入输出皆为结构化数据)，导致我们严格了输入输出流，将数据源中的所有数据都扁平化，如不允许树状结构出现。尽管到现在为止的业务需求上也没啥问题，但是从理论的完整性上，引入HBase确不失为预备之举。

2. 计算系统的输出任务结果为何不使用redis存放？
    - 1) 如果使用单机版redis，则形成了分布式系统的庞大数据流与redis吞吐量之间的不对等，可能影响写入速度
    - 2) 采用集群redis，尚不知redis集群版成熟度以及与Spark这样的系统间的适应度
    - 3) 采用HBase可以存放历史版本的推送任务，即那些可能已经被执行完毕的过期任务。用来干啥呢？用来分析学习啊，看看推送方案的历史演进，以及历史版本的推送任务与其实际执行效果间的反馈。总之，我们尽可能保留任何可能有价值的数据。。。。。。。。。。



## 未完设计
- 完善的监控系统。根据系统监控数据、业务运行日志、服务器错误日志、关键业务指标等做的实时行为检测，实时预警和报警，实时展示，实时切换PlanB。
- 手工控制的控制台。想要达到完美程度需要长期的运营经验，而且即便做的足够好也不可避免的有意料之外的事情发生，所以关键服务器需要提供控制接口，以便人工干预，参数控制
- 运营系统。
1.  如录入文件的内容描述，以供数据挖掘使用
2.  人工导入从某些渠道获知的节目单，控制系统提前下发某些任务，比如已知的电视剧集更新，直播预告等等
    

      


